---
title: On Evaluating Adversarial Robustness
summary: Studied the principles that why the attack and defense work in a certain problem setup and showed the advantages and disadvantages of different defense models.


tags:
- Deep Learning
date: "2021-05-07T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: ""


image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart

#links:
#- icon: twitter
#  icon_pack: fab
#  name: Follow
#  url: https://twitter.com/georgecushen
  
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
#slides: example
---

{{< icon name="download" pack="fas" >}} Download {{< staticref "media/DSCI552_Project.pdf" "newtab" >}}Project{{< /staticref >}}.

## Abstract

Machine learning’s current driving force is to generate more accurate models rather than focusing on models’ security and robustness. In reality, many machine learning models are vulnerable to different adversarial attacks. This paper will discuss how to evaluate model robustness based on the three threat models and three defense models. We will also introduce evaluation metrics and give numerical examples using MNIST handwriting, MNIST fashion, and the CIFAR-10 dataset. In our attack model experiment, we confirm that l2 will bring stronger perturb on images. The range of epsilon also will be varied on the nature of the image and the distance metrics. We also confirm that both adversarial training and pixel defend models are helpful in the image classification task from the defense model experiment.


**Keywords** Machine Learning, Model Robustness, Adversarial Training, Gradient Masking, Reactive Defense


